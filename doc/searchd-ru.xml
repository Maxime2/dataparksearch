<sect1 id="searchd">
	<title>Поддержка SearchD
<indexterm><primary>searchd</primary></indexterm>
</title>
	<para><command>searchd</command> предназначен для ускорения поиска, особенно при использовании ispell-данных,
синонимов и стоп-слов. </para>

	<sect2 id="searchd-why">
		<title>Для чего использовать <command>searchd</command></title>
		<itemizedlist>
			<listitem>
				<para>Для быстрого поиска, особенно если используется нечёткий поиск на основе ispell или 
синонимов, а также сегментеры для азиатских языков. 
Необходимые файлы загружаются в память однажды при запуске <command>searchd</command>, в то время как
<filename>search.cgi</filename> без использования <command>searchd</command> загружает эти данные для каждого запроса.</para>

<para>Таже <command>searchd</command> может загружать предварительно в память некоторые данные об URL 
(по 20 байт на каждую проиндекированую страницу) и лимиты cache mode (4 или 8 байт на каждый URL в зависимости от
типа лимита).
</para>
			</listitem>
			<listitem>
				<para>Для возможности разнесения поискового и веб-серверов на разные машины.</para>
			</listitem>
		</itemizedlist>
	</sect2>

	<sect2 id="searchd-start">
		<title>Запуск <command>searchd</command></title>
		<para>Для запуска <command>searchd</command> проделайте следующее:</para>
		<itemizedlist>
			<listitem>
				<para>Скопируйте <filename>$PREFIX/etc/searchd.conf-dist</filename> в <filename>searchd.conf</filename>.</para>
			</listitem>
			<listitem>
				<para>Отредактируйте <filename>searchd.conf</filename>.</para>
			</listitem>
<listitem>
<indexterm><primary>Команда</primary><secondary>PreloadURLData</secondary></indexterm>
<para>Если ускорения поиска вы хотите для загрузить в память и информацию об url (примерно по 20 байт на url),
добавьте в <filename>searchd.conf</filename> следующую команду:
				<programlisting>
PreloadURLData yes
</programlisting>
</para>
</listitem>
<listitem>
<indexterm><primary>Команда</primary><secondary>PreloadLimit</secondary></indexterm>
<para>Вы также можете загрузить в память индексы для наиболее часто исползуемых значений лимитов режима хранения cache используя
команду <command>PreloadLimit</command> в файде <filename>searchd.conf</filename>:
				<programlisting>
PreloadLimit &lt;limit type&gt; &lt;limit value&gt;
</programlisting>
</para>
<para>Например:
			<programlisting>
PreloadLimit tag Unix
</programlisting>
</para>
</listitem>

			<listitem>
				<para>Добавьте следующую команду в <filename>search.htm</filename>:</para>
				<para>
<literal>DBAddr searchd://hostname/</literal> или <literal>DBAddr searchd://hostname:port/</literal>, например:
				<programlisting>
DBAddr searchd://localhost/
</programlisting>
</para>
				<para>Значение по умолчанию для <literal>port</literal> равно 7003</para>
			</listitem>

<listitem>
<para><indexterm><primary>Команда</primary><secondary>MaxClients</secondary></indexterm>
Вы можете запустить несколько чилдов searchd, отвечающих на поисковые заапросы параллельно. 
Используете команду <command>MaxClients</command> для задания этого числа. Значение по умолчанию: 1.
</para>
<programlisting>
MaxClients 2
</programlisting>
</listitem>


			<listitem>
				<para>Запустите <command>searchd</command>:</para>
				<para>
					<userinput>/usr/local/dpsearch/sbin/searchd &amp;</userinput>
				</para>
			</listitem>
		</itemizedlist>

		<para>Чтобы обойтись без вывода на stderr, используйте ключ
<literal>-l</literal>. Вывод сообщений в этом слуае будет происходить только через syslog
(если поддержка syslog не была выключена при инсталяции при помощи ключа
<literal>--disable-syslog</literal>). В случае, если поддержка syslog выключена, можно
перенаправить stderr в файл: </para>
		<para>
			<userinput>/usr/local/dpsearch/sbin/searchd 2&gt;/var/log/searchd.log &amp;</userinput>
		</para>
		<para>
			Для <literal>searchd</literal>, так же как и для
<filename>indexer</filename> можно указывать имя файла конфигурации в качестве параметра, например,
относительно поддиректории <filename>/etc</filename>
корневой директории установки <application>DataparkSearch</application>:
		<programlisting>
searchd searchd1.conf
</programlisting>
		</para>
		<para>или указав абсолютный путь:</para>
		<para>
			<userinput>searchd /usr/local/dpsearch/etc/searchd1.conf</userinput>
		</para>
	</sect2>

	<!-- sect2 id="searchd-merge">
		<title>Merging several databases</title>
		<para>It is possible to indicate several
<command>DBAddr</command> commands in
<filename>search.htm</filename>. In this case
<filename>search.cgi</filename> will send queries via TCP/IP to
several searchd's and compile results. In version 3.2.0 up to 256
databases are supported. <varname>DBMode</varname> and type of
databases may differ with various
searchd's. </para>
		<para>
			<filename>search.cgi</filename> starts with
sending queries to every searchd, thus activating parallel searches in
every searchd. Then it waits for the results, compiles them and
selects best matches. </para>
		<para>Thus it is possible to create a distributed
across several machines database. Please note that databases should
not intersect, i.e. same documents should not be present in several
merged databases. Otherwise the document will be duplicated in search
results.</para>
	</sect2>
	<sect2 id="searchd-distribute">
		<title>Distributed indexing</title>
		<para>Indexing distribution can be done by means of hostname filtering.</para>
		<para>Imagine it is necessary to create a search
engine, e.g. for .de domain. Search administrator has 28 machines
available, and their names for example are:
		<programlisting>
a.hostname.de
b.hostname.de
...
...
z.hostname.de
</programlisting>
</para>
		<para>
			<filename>indexer.conf</filename> is created for every machine. E.g. on a machine <literal>a.hostname.de</literal>:
		<programlisting>
# For hostnames starting with www:
Realm http://www.a*.de/

# For hostnames without www:
Realm http://a*.de/
</programlisting>
		</para>
		<para>Repeat this action for every machine.</para>
		<para>Searchd understands the following commands in
<filename>searchd.conf</filename> as well, they are similar to those
in <filename>indexer.conf</filename>.
		<programlisting>
Allow x.x.x.x
Disallow x.x.x.x
</programlisting>
		</para>
		<para>With the above commands you may specify which
hosts can/can not  connect to searchd. In case the commands are not
specified, any host can connect. E.g. to allow connecting from
<literal>localhost</literal> only:
		<programlisting>
Allow     127.0.0.1
Disallow  *
</programlisting>
		</para>
		<para>Or from the 192.168.x.x network only:
		<programlisting>
Allow 192.168.*.*
Disallow *
</programlisting>
		</para>
		<para>To make searchd reload the configuration file with the HUP signal, use the following command:
		<programlisting>
kill -HUP xxx
</programlisting>
		</para>
		<para>Where <literal>xxx</literal> - id number of the process (pid).</para>
		<para>Then <literal>indexer</literal> is run on every machine (or several indexers) that index their own area. </para>
		<para>A <filename>search.cgi</filename> is installed
on every machine and the following lines are added to every
corresponding template:
		<programlisting>
DBAddr searchd://a.hostname.de
DBAddr searchd://b.hostname.de
....
DBAddr searchd://z.hostname.de
</programlisting>
</para>
		<para>Thus <filename>search.cgi</filename> will send
parallel queries to every machine and return best results to
user.</para>
		<para>In the current version indexing of each area is
done independently. If on the server
<literal>http://a.domane.de/</literal> there is a link to
<literal>http://b.doname.de/</literal> server, this link will not be
transferred from the machine responsible for a to the machine
responsible for b.</para>
		<para>Since distribution by hostname is used, in case
one of the machines is not operational, the information of all the web
servers that were indexed on this machine will be unavailable. </para>
		<para>It is planned to implement in the future
versions communication between "neighbouring" hosts (i.e. the hosts
will be able to transfer links between each other, as well as other
types of distribution - by hash-function from document's URL. That
means that one site's pages will be evenly distributed by all the
machines of the cluster. So in case one of the machines is
unavailable, all the sites will still be available on other
machines.</para>
	</sect2 -->
</sect1>
